{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import helper_functions.ppc as ppc\n",
    "import pyro\n",
    "from pyro import plate\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.autoguide as autoguide\n",
    "from pyro.infer import MCMC, NUTS, SVI, Trace_ELBO, Predictive\n",
    "import pyro.optim as optim\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import folium\n",
    "import seaborn as sns\n",
    "from folium.plugins import FastMarkerCluster, HeatMap\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from torch.distributions.constraints import positive\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "FIRST_DAY = datetime.datetime(2014,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will fit a simple linear model to our data to see how it performs. This will consist of no structured componenets, nor any additional covariates or predictors. It will be used as the baseline with respect to which we can evaluate all of our additional models. To do so first we will take a quick look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``datetime`` and ``node`` cols \n",
    "    Into [day,node] nd.array where arr[i][j] indicates\n",
    "    accidents in location i at j day \n",
    "    Returns accident array\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    categorical = pd.Categorical(df['node'])\n",
    "    codes = categorical.codes\n",
    "\n",
    "    num_days = (df['datetime'].iloc[-1] - FIRST_DAY).days + 1\n",
    "    num_nodes = len(categorical.categories)\n",
    "\n",
    "    data_arr = np.zeros((num_nodes, num_days))\n",
    "    for elem, i in zip(df.itertuples(),range(len(df))): \n",
    "        data_arr[codes[i]][(elem.datetime - FIRST_DAY).days] += 1\n",
    "    \n",
    "    category_mapping = {}\n",
    "    for node, idx in zip(categorical, categorical.codes):\n",
    "        category_mapping[node] = idx \n",
    "    \n",
    "    return num_days, category_mapping, data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_filename = '../data/manhattan_accidents_node_data.csv'\n",
    "node_filename = '../data/nodes_roads.csv'\n",
    "\n",
    "\n",
    "data = pd.read_csv(accident_filename)\n",
    "node_data = pd.read_csv(node_filename)\n",
    "\n",
    "#Only consider accident with node that have corresponding AADT\n",
    "data = data[data['node'].isin(list(node_data['nodes'].unique()))]\n",
    "\n",
    "#Only consider accidents after 2014\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data = data[data['datetime'] >= FIRST_DAY]\n",
    "\n",
    "#This is done to get rid of pesky column. \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can divide the data between a training and test set. To do this we can simply take the time up to year 2016 and then consider everything else validation.  Moreover we will re-organize our data so that we can work with it a comfortable way later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_days, categorical_mapping, data_mat = transform_dataframe(data)\n",
    "\n",
    "num_years_test = 2\n",
    "num_years_train = 3\n",
    "\n",
    "index_train = 365 * num_years_train\n",
    "index_valid = index_train + 365 * num_years_test\n",
    "\n",
    "train_mat = data_mat[:, :index_train]\n",
    "test_mat = data_mat[:, index_train: index_valid]\n",
    "data_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data that we have we can plot several visualizations. First we will plot the points in the map so that we can see what type of data we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_child(FastMarkerCluster(subset))\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being able to visualize the points independently it is also helpful to visualize the points in a heat map to understand if there are any spatial correlations that we should be mindful of. As the map shows, there are definitely some areas that seem to be more likely to have accidents than others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_children(HeatMap(subset, radius = 7.5))\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to understand and which will be key later on is whether there exists some general trends on accidents through time. Of course, it is possible that for some sites some of these trends are present while for some others these trends don't exist. However, for the moment we will only consider whether for manhattan as whole these trends exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_accidents = np.sum(data_mat, axis = 0)\n",
    "smooth_accidents = signal.savgol_filter(time_accidents,61, 3)\n",
    "time = list(range(len(time_accidents)))\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time,y=time_accidents, mode='lines',name='Raw accidents'))\n",
    "fig.add_trace(go.Scatter(x=time,y=smooth_accidents,mode='lines',name='Smooth accidents'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'mean accidents':mean_accidents}), x='mean accidents', kind = 'kde')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'log mean accidents':np.log(mean_accidents + 0.000000000001/len(mean_accidents))}), x='log mean accidents', kind = 'kde', ax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify our model. Our first model is that both the risk and exposure at each individual site will be constant through time. This is a very crude approximation but it will serve as a simple baseline that we can use for our evaluation. Mathematically, we can state is as follows. Let $j \\in [n]$ where $n$ denotes the number of days. Let $i \\in [m]$ where $m$ denotes the number of sites. Let $Y_{ij}$ the number of accidents at site $i$ during during day $j$. Then we assume that \n",
    "\\begin{align*}\n",
    "\\beta &\\sim \\mathcal{N}(0,I_m)\\\\\n",
    "Y_{ij} &\\sim \\text{Poisson}(\\exp(\\beta_i))\n",
    "\\end{align*}\n",
    "Having specified this, we can simply write down our model and then do inference. Because the betas are independent here, then it would make sense to specify a guide where all of the parameters are normal. This can be done quite easily with an autoguide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = train_mat.shape[0]\n",
    "num_days = train_mat.shape[1]\n",
    "\n",
    "def base_model(num_sites, num_days, data):\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(-5, 3))\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(epsilon)), obs=data)\n",
    "            \n",
    "    return accidents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = autoguide.AutoDiagonalNormal(base_model)\n",
    "optimizer = optim.Adam({'lr': .05})\n",
    "num_iters = 500\n",
    "\n",
    "svi = SVI(base_model, guide, optimizer,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'Loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='Loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the next step is to import predictor data. This would include both intersection-level data such as AADT and day-level data such as weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_nodes_dataframe(categorical_mapping, df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``node`` and corresponding predictor cols \n",
    "    Into [day,node,predictor] nd.array where arr[j][i][k] indicates\n",
    "    predictor k in location i at j day \n",
    "    Returns predictors tensor\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    df = df.replace({'nodes': categorical_mapping})\n",
    "\n",
    "    num_nodes = len(categorical_mapping.keys())\n",
    "    \n",
    "    data_arr = np.zeros((num_nodes, num_days, 2))\n",
    "    for i in range(len(df)): \n",
    "        if df['nodes'][i] >= 0 and df['nodes'][i] <= data_arr.shape[0]:\n",
    "            data_arr[df['nodes'][i], :, 0] = 1\n",
    "            data_arr[df['nodes'][i], :, 1] = df['Count_mean'][i]\n",
    "    return data_arr\n",
    "\n",
    "\n",
    "predictors = transform_nodes_dataframe(categorical_mapping, node_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we use now is gonna be different. Before, we had all of our risk and exposure at a crash site modeled by one term per site. Here we introduce several modifications and choose the following model instead. As before, let $Y_{ij}$ denote the number of car crashes at site $i$ on day $j$. Now we introduce a variable $theta_{ij}$  which we will call the risk at site $i$ on day $j$ of a crash and a variable $\\lambda_{ij}$ which we call the exposure at site $i$ on day $j$. This exporsure, is associated with the number of cars that pass through a given site on a day. Intuitively, the more cars that pass through some site on a day. Our model then is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors[:,:,1] = np.log(predictors[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aadt_model(num_sites, num_days, num_predictors,predictors, data=None):\n",
    "    betas = pyro.sample('betas', dist.Normal(0 * torch.ones(num_predictors), 10 * torch.ones(num_predictors)))\n",
    "    thetas = predictors @ betas\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(0, 5)).expand(num_sites, num_days)\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            thetas = thetas + epsilon\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(thetas)), obs=data) \n",
    "\n",
    "    return accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadt_model_guide = autoguide.AutoDiagonalNormal(aadt_model)\n",
    "num_iters = 1000\n",
    "optimizer = torch.optim.Adam\n",
    "optim_args = {'lr': 0.1}\n",
    "optim_params = {\n",
    "    'optimizer': optimizer,\n",
    "    'optim_args':optim_args, \n",
    "    'step_size': 200,\n",
    "    'gamma':0.1,\n",
    "    'verbose':True}\n",
    "scheduler = optim.StepLR(optim_params)\n",
    "\n",
    "svi = SVI(aadt_model, aadt_model_guide, scheduler,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "predictors_tens = torch.tensor(predictors).float()\n",
    "covariates = predictors_tens\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, 2, predictors_tens, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'log-loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='log-loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should go in an independent file \n",
    "from collections import defaultdict\n",
    "from pyro import poutine\n",
    "from pyro.poutine.util import prune_subsample_sites\n",
    "import warnings\n",
    "\n",
    "class Predict(torch.nn.Module):\n",
    "    def __init__(self, model, guide, num_samples=800):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        total_samples = {}\n",
    "        for i in range(self.num_samples): \n",
    "            if i % 50 == 0: \n",
    "                print(\"done with {}\".format(i))\n",
    "            guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)\n",
    "            model_trace = poutine.trace(poutine.replay(self.model, guide_trace)).get_trace(*args, **kwargs)\n",
    "            for site in prune_subsample_sites(model_trace).stochastic_nodes:\n",
    "                if site not in total_samples: \n",
    "                    total_samples[site] = []\n",
    "                total_samples[site].append(model_trace.nodes[site]['value'])\n",
    "        for key in total_samples.keys():\n",
    "            total_samples[key] = torch.stack(total_samples[key])\n",
    "\n",
    "        return total_samples\n",
    "    \n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = Predict(aadt_model,aadt_model_guide, 800)\n",
    "samples = predict(num_sites, num_days, num_predictors, predictors_tens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(samples['betas'][:,0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = torch.distributions.Categorical(torch.ones(4,num_sites)/num_sites).sample()\n",
    "print(subset)\n",
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(),train_mat,(2,2),subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_max(samples['accidents'].detach().numpy(), train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_predictors(predictors, df):\n",
    "    df = df.copy()\n",
    "    new_predictors = np.zeros((predictors.shape[0], predictors.shape[1], 6))\n",
    "    df['datetime'] = pd.to_datetime(df['DATE'])\n",
    "    for elem in df.itertuples():\n",
    "        idx = (elem.datetime - FIRST_DAY).days\n",
    "        \n",
    "        if idx >= 0 and idx < new_predictors.shape[1]:\n",
    "            new_predictors[:, idx, 2:] = [elem.AWND, elem.PRCP, elem.SNWD, elem.TAVG]\n",
    "    new_predictors[:, :, :2] = predictors\n",
    "    return new_predictors\n",
    "weather_data = pd.read_csv('../data/weather.csv')\n",
    "new_predictors = add_weather_predictors(predictors, weather_data)\n",
    "\n",
    "# Step to normalize the variables to be between -1 and 1\n",
    "new_predictors = new_predictors/np.max(np.abs(new_predictors[0]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadt_model_guide = autoguide.AutoDiagonalNormal(aadt_model)\n",
    "num_iters = 2000\n",
    "optimizer = torch.optim.Adam\n",
    "optim_args = {'lr': 0.05}\n",
    "optim_params = {\n",
    "    'optimizer': optimizer,\n",
    "    'optim_args':optim_args, \n",
    "    'step_size': 200,\n",
    "    'gamma':0.1,\n",
    "    'verbose':True}\n",
    "scheduler = optim.StepLR(optim_params)\n",
    "\n",
    "svi = SVI(aadt_model, aadt_model_guide, scheduler,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "predictors_tens = torch.tensor(new_predictors).float()\n",
    "num_predictors = predictors_tens.shape[-1]\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, num_predictors, predictors_tens, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(aadt_model,aadt_model_guide, 800)\n",
    "samples = predict(num_sites, num_days, num_predictors, predictors_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = torch.distributions.Categorical(torch.ones(4,num_sites)/num_sites).sample()\n",
    "print(subset)\n",
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(),train_mat,(2,2),subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(samples['betas'][:,2].detach().numpy(),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_max(samples['accidents'].detach().numpy(),train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(samples['accidents'].detach().numpy(),train_mat, window=31, polynomial=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ppc plots it is evident that there are several things that could be improved. On the one hand we can see that our model is not good enough at producing the high crash results that we see in the data. This indicates that the distribution that we use to model the crashes should perhaps not be Poisson. Additionally, we see that the weather is not enough to capture all of the seasonality that we see. These two insights by the ppc plots can help us revisit our model. We will go with both modifications separately. First, we will us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(train_mat)/(train_mat.shape[0] * train_mat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_samples = samples['accidents'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [np.count_nonzero(elem)/(train_mat.shape[0] * train_mat.shape[1]) for elem in accidents_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(vals,kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_vals = np.count_nonzero(train_mat, axis=1)\n",
    "vals = [np.count_nonzero(elem, axis =1) for elem in accidents_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = int(np.random.rand(1)*2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(vals[:,random],kde=True)\n",
    "print(real_vals[random])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the heavier tails\n",
    "As we can see, we do have heavier tails than the poisson distribution would lead us to believe. One possible solution is to use a negative binomial distribution. This distribution resembles the poisson (in fact the poisson is a limiting case of it) but has more parameters that allow us to control the dispersion. In particular, it can be parametrized by the mean of the distribution and a parameter $p \\in [0,1]$ (this parametrization is not standard but is useful for our purspose). Under this paramterization it can be shown that the excess kurtosis, which is a measurment of the heaviness of the tails, is given by \n",
    "$$ \\frac{1}{\\lambda} \\frac{1}{1/p -1}(6 + (1-p)^2)$$\n",
    "for comparison, the poisson distribution has an exess kurtosis of $1/\\lambda$ only. Thus, for our model we keep everyhting like we had before and we will continue to model the mean with the same process but we will add this parameter $p$ with a uniform prior over $[0,1]$. In particular we will choose one $p$ for each of the different sites as we would expect that the kurtosis would be site dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_binomial_model(num_sites, num_days, num_predictors,predictors, data=None):\n",
    "    with plate(\"betas_plates\", num_predictors): \n",
    "        betas = pyro.sample('betas', dist.Normal(torch.zeros(num_predictors), 10 * torch.ones(num_predictors)))\n",
    "        \n",
    "    with plate('sites_params', num_sites): \n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(torch.zeros(num_sites), 5))\n",
    "        epsilon = epsilon.unsqueeze(-1).expand(num_sites, num_days)\n",
    "        theta = torch.exp(predictors @ betas + epsilon)\n",
    "        p = pyro.sample('p', dist.Beta(1000,10))\n",
    "        p = p.unsqueeze(-1).expand(num_sites, num_days)\n",
    "        r = ((1-p)/p)*theta\n",
    "        \n",
    "        \n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            accidents = pyro.sample('accidents', dist.NegativeBinomial(r,p), obs=data) \n",
    "            \n",
    "    return accidents\n",
    "\n",
    "def negative_binomial_guide(num_sites, num_days, num_predictors, predictors, data=None):\n",
    "    #Parameters for p\n",
    "    alpha_0 = pyro.param('alpha_0', 5*torch.ones(num_sites), constraint =positive)\n",
    "    alpha_1 = pyro.param('alpha_1', 5*torch.ones(num_sites), constraint =positive)\n",
    "    \n",
    "    means_epsilon = pyro.param('means_epsilon', torch.zeros(num_sites))\n",
    "    means_betas = pyro.param('means_betas', torch.zeros(num_predictors))\n",
    "    variance_epsilon = pyro.param('variance_epsilon', torch.ones(num_sites), constraint=positive)\n",
    "    variance_betas = pyro.param('variance_beta', torch.ones(num_predictors), constraint=positive)\n",
    "    \n",
    "    with plate(\"beta_plates\", num_predictors):\n",
    "        pyro.sample('betas', dist.Normal(means_betas, variance_betas))\n",
    "    \n",
    "    with plate('sites',size=num_sites): \n",
    "        pyro.sample('epsilon',dist.Normal(means_epsilon, variance_epsilon))\n",
    "        pyro.sample('p', dist.Beta(alpha_0,alpha_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 100\n",
    "optimizer = torch.optim.Adam\n",
    "optim_args = {'lr': 0.1}\n",
    "optim_params = {\n",
    "    'optimizer': optimizer,\n",
    "    'optim_args':optim_args, \n",
    "    'step_size': 10,\n",
    "    'gamma':0.1,\n",
    "    'verbose':True}\n",
    "scheduler = optim.StepLR(optim_params)\n",
    "\n",
    "svi = SVI(negative_binomial_model,negative_binomial_guide, scheduler,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "predictors_tens = torch.tensor(new_predictors).float()\n",
    "num_predictors = predictors_tens.shape[-1]\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, num_predictors, predictors_tens, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 10 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'log-loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='log-loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(negative_binomial_model,negative_binomial_guide, 200)\n",
    "samples = predict(num_sites, num_days, num_predictors, predictors_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = torch.distributions.Categorical(torch.ones(4,num_sites)/num_sites).sample()\n",
    "print(subset)\n",
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(),train_mat,(2,2),subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(samples['p'][:,4].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(0.001,0.4,1000)\n",
    "def f(x):\n",
    "    return (x/(1-x))*(6 + (1-x)**2)\n",
    "plt.plot(x,f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import nbinom, poisson\n",
    "mean = 150\n",
    "p = 0.8\n",
    "r = (mean * p)/(1-p)\n",
    "size =10000\n",
    "nbinomvar = nbinom.rvs(r,p,size=size)\n",
    "poissonvar = poisson.rvs(mean,size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(nbinomvar,kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(poissonvar, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(train_mat, axis=0)\n",
    "b = np.zeros(7)\n",
    "\n",
    "for i in range(len(a)):\n",
    "    b[i%7] = b[i%7] + a[i]\n",
    "        \n",
    "    \n",
    "plt.plot(list(range(7)),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
