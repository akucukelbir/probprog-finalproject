{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyro\n",
    "from pyro import plate\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.autoguide as autoguide\n",
    "from pyro.infer import MCMC, NUTS, SVI, Trace_ELBO\n",
    "import pyro.optim as optim\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import folium\n",
    "import seaborn as sns\n",
    "from folium.plugins import FastMarkerCluster, HeatMap\n",
    "\n",
    "FIRST_DAY = datetime.datetime(2014,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will fit a simple linear model to our data to see how it performs. This will consist of no structured componenets, nor any additional covariates or predictors. It will be used as the baseline with respect to which we can evaluate all of our additional models. To do so first we will take a quick look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``datetime`` and ``node`` cols \n",
    "    Into [day,node] nd.array where arr[i][j] indicates\n",
    "    accidents in location i at j day \n",
    "    Returns accident array\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    categorical = pd.Categorical(df['node'])\n",
    "    codes = categorical.codes\n",
    "\n",
    "    num_days = (df['datetime'].iloc[-1] - FIRST_DAY).days + 1\n",
    "    num_nodes = len(categorical.categories)\n",
    "\n",
    "    data_arr = np.zeros((num_nodes, num_days))\n",
    "    for elem, i in zip(df.itertuples(),range(len(df))): \n",
    "        data_arr[codes[i]][(elem.datetime - FIRST_DAY).days] += 1\n",
    "    \n",
    "    category_mapping = {}\n",
    "    for node, idx in zip(categorical, categorical.codes):\n",
    "        category_mapping[node] = idx \n",
    "    \n",
    "    return num_days, category_mapping, data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65727</th>\n",
       "      <td>257388</td>\n",
       "      <td>2014-01-01 00:01:00</td>\n",
       "      <td>-73.981512</td>\n",
       "      <td>40.767889</td>\n",
       "      <td>4347550071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65728</th>\n",
       "      <td>257389</td>\n",
       "      <td>2014-01-01 00:01:00</td>\n",
       "      <td>-73.978608</td>\n",
       "      <td>40.750844</td>\n",
       "      <td>561042199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65729</th>\n",
       "      <td>257390</td>\n",
       "      <td>2014-01-01 00:01:00</td>\n",
       "      <td>-73.996771</td>\n",
       "      <td>40.725432</td>\n",
       "      <td>1919595915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65731</th>\n",
       "      <td>257412</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>-73.965784</td>\n",
       "      <td>40.758633</td>\n",
       "      <td>42442960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65732</th>\n",
       "      <td>257414</td>\n",
       "      <td>2014-01-01 02:02:00</td>\n",
       "      <td>-73.944677</td>\n",
       "      <td>40.791570</td>\n",
       "      <td>42450057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335808</th>\n",
       "      <td>1520426</td>\n",
       "      <td>2020-10-30 20:15:00</td>\n",
       "      <td>-73.988520</td>\n",
       "      <td>40.745200</td>\n",
       "      <td>42428223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335809</th>\n",
       "      <td>1520434</td>\n",
       "      <td>2020-10-30 20:44:00</td>\n",
       "      <td>-73.949486</td>\n",
       "      <td>40.772705</td>\n",
       "      <td>42428024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335810</th>\n",
       "      <td>1520445</td>\n",
       "      <td>2020-10-30 21:46:00</td>\n",
       "      <td>-73.975840</td>\n",
       "      <td>40.748863</td>\n",
       "      <td>42445661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335811</th>\n",
       "      <td>1520450</td>\n",
       "      <td>2020-10-30 22:15:00</td>\n",
       "      <td>-73.920616</td>\n",
       "      <td>40.866760</td>\n",
       "      <td>42431242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335812</th>\n",
       "      <td>1520461</td>\n",
       "      <td>2020-10-30 23:45:00</td>\n",
       "      <td>-73.988365</td>\n",
       "      <td>40.720074</td>\n",
       "      <td>42437096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220355 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            datetime  longitude   latitude        node\n",
       "65727       257388 2014-01-01 00:01:00 -73.981512  40.767889  4347550071\n",
       "65728       257389 2014-01-01 00:01:00 -73.978608  40.750844   561042199\n",
       "65729       257390 2014-01-01 00:01:00 -73.996771  40.725432  1919595915\n",
       "65731       257412 2014-01-01 02:00:00 -73.965784  40.758633    42442960\n",
       "65732       257414 2014-01-01 02:02:00 -73.944677  40.791570    42450057\n",
       "...            ...                 ...        ...        ...         ...\n",
       "335808     1520426 2020-10-30 20:15:00 -73.988520  40.745200    42428223\n",
       "335809     1520434 2020-10-30 20:44:00 -73.949486  40.772705    42428024\n",
       "335810     1520445 2020-10-30 21:46:00 -73.975840  40.748863    42445661\n",
       "335811     1520450 2020-10-30 22:15:00 -73.920616  40.866760    42431242\n",
       "335812     1520461 2020-10-30 23:45:00 -73.988365  40.720074    42437096\n",
       "\n",
       "[220355 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accident_filename = '../data/manhattan_accidents_node_data.csv'\n",
    "node_filename = '../data/nodes_roads.csv'\n",
    "\n",
    "\n",
    "data = pd.read_csv(accident_filename)\n",
    "node_data = pd.read_csv(node_filename)\n",
    "\n",
    "#Only consider accident with node that have corresponding AADT\n",
    "data = data[data['node'].isin(list(node_data['nodes'].unique()))]\n",
    "\n",
    "#Only consider accidents after 2014\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data = data[data['datetime'] >= FIRST_DAY]\n",
    "\n",
    "#This is done to get rid of pesky column. \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can divide the data between a training and test set. To do this we can simply take the time up to year 2016 and then consider everything else validation.  Moreover we will re-organize our data so that we can work with it a comfortable way later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_days, categorical_mapping, data_mat = transform_dataframe(data)\n",
    "\n",
    "num_years_test = 2\n",
    "num_years_train = 3\n",
    "\n",
    "index_train = 365 * num_years_train\n",
    "index_valid = index_train + 365 * num_years_test\n",
    "\n",
    "train_mat = data_mat[:, :index_train]\n",
    "test_mat = data_mat[:, index_train: index_valid]\n",
    "data_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data that we have we can plot several visualizations. First we will plot the points in the map so that we can see what type of data we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_child(FastMarkerCluster(subset))\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being able to visualize the points independently it is also helpful to visualize the points in a heat map to understand if there are any spatial correlations that we should be mindful of. As the map shows, there are definitely some areas that seem to be more likely to have accidents than others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_children(HeatMap(subset, radius = 7.5))\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to understand and which will be key later on is whether there exists some general trends on accidents through time. Of course, it is possible that for some sites some of these trends are present while for some others these trends don't exist. However, for the moment we will only consider whether for manhattan as whole these trends exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_accidents = np.sum(data_mat, axis = 0)\n",
    "smooth_accidents = signal.savgol_filter(time_accidents,61, 3)\n",
    "time = list(range(len(time_accidents)))\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time,y=time_accidents, mode='lines',name='Raw accidents'))\n",
    "fig.add_trace(go.Scatter(x=time,y=smooth_accidents,mode='lines',name='Smooth accidents'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'mean accidents':mean_accidents}), x='mean accidents', kind = 'kde')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'log mean accidents':np.log(mean_accidents + 0.000000000001/len(mean_accidents))}), x='log mean accidents', kind = 'kde', ax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify our model. Our first model is that both the risk and exposure at each individual site will be constant through time. This is a very crude approximation but it will serve as a simple baseline that we can use for our evaluation. Mathematically, we can state is as follows. Let $j \\in [n]$ where $n$ denotes the number of days. Let $i \\in [m]$ where $m$ denotes the number of sites. Let $Y_{ij}$ the number of accidents at site $i$ during during day $j$. Then we assume that \n",
    "\\begin{align*}\n",
    "\\beta &\\sim \\mathcal{N}(0,I_m)\\\\\n",
    "Y_{ij} &\\sim \\text{Poisson}(\\exp(\\beta_i))\n",
    "\\end{align*}\n",
    "Having specified this, we can simply write down our model and then do inference. Because the betas are independent here, then it would make sense to specify a guide where all of the parameters are normal. This can be done quite easily with an autoguide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = train_mat.shape[0]\n",
    "num_days = train_mat.shape[1]\n",
    "\n",
    "def base_model(num_sites, num_days, data):\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(-5, 3))\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(epsilon)), obs=data)\n",
    "            \n",
    "    return accidents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = autoguide.AutoDiagonalNormal(base_model)\n",
    "optimizer = optim.Adam({'lr': .05})\n",
    "num_iters = 5000\n",
    "\n",
    "svi = SVI(base_model, guide, optimizer,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'Loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='Loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the next step is to import predictor data. This would include both intersection-level data such as AADT and day-level data such as weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e+00, 6.53200000e+03],\n",
       "        [1.00000000e+00, 6.53200000e+03],\n",
       "        [1.00000000e+00, 6.53200000e+03],\n",
       "        ...,\n",
       "        [1.00000000e+00, 6.53200000e+03],\n",
       "        [1.00000000e+00, 6.53200000e+03],\n",
       "        [1.00000000e+00, 6.53200000e+03]],\n",
       "\n",
       "       [[1.00000000e+00, 9.97433333e+03],\n",
       "        [1.00000000e+00, 9.97433333e+03],\n",
       "        [1.00000000e+00, 9.97433333e+03],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.97433333e+03],\n",
       "        [1.00000000e+00, 9.97433333e+03],\n",
       "        [1.00000000e+00, 9.97433333e+03]],\n",
       "\n",
       "       [[1.00000000e+00, 2.07126667e+04],\n",
       "        [1.00000000e+00, 2.07126667e+04],\n",
       "        [1.00000000e+00, 2.07126667e+04],\n",
       "        ...,\n",
       "        [1.00000000e+00, 2.07126667e+04],\n",
       "        [1.00000000e+00, 2.07126667e+04],\n",
       "        [1.00000000e+00, 2.07126667e+04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e+00, 9.60300000e+03],\n",
       "        [1.00000000e+00, 9.60300000e+03],\n",
       "        [1.00000000e+00, 9.60300000e+03],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.60300000e+03],\n",
       "        [1.00000000e+00, 9.60300000e+03],\n",
       "        [1.00000000e+00, 9.60300000e+03]],\n",
       "\n",
       "       [[1.00000000e+00, 6.26400000e+03],\n",
       "        [1.00000000e+00, 6.26400000e+03],\n",
       "        [1.00000000e+00, 6.26400000e+03],\n",
       "        ...,\n",
       "        [1.00000000e+00, 6.26400000e+03],\n",
       "        [1.00000000e+00, 6.26400000e+03],\n",
       "        [1.00000000e+00, 6.26400000e+03]],\n",
       "\n",
       "       [[1.00000000e+00, 8.20357500e+04],\n",
       "        [1.00000000e+00, 8.20357500e+04],\n",
       "        [1.00000000e+00, 8.20357500e+04],\n",
       "        ...,\n",
       "        [1.00000000e+00, 8.20357500e+04],\n",
       "        [1.00000000e+00, 8.20357500e+04],\n",
       "        [1.00000000e+00, 8.20357500e+04]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_nodes_dataframe(categorical_mapping, df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``node`` and corresponding predictor cols \n",
    "    Into [day,node,predictor] nd.array where arr[j][i][k] indicates\n",
    "    predictor k in location i at j day \n",
    "    Returns predictors tensor\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    df = df.replace({'nodes': categorical_mapping})\n",
    "\n",
    "    num_nodes = len(categorical_mapping.keys())\n",
    "    \n",
    "    data_arr = np.zeros((num_nodes, num_days, 2))\n",
    "    for i in range(len(df)): \n",
    "        if df['nodes'][i] >= 0 and df['nodes'][i] <= data_arr.shape[0]:\n",
    "            data_arr[df['nodes'][i], :, 0] = 1\n",
    "            data_arr[df['nodes'][i], :, 1] = df['Count_mean'][i]\n",
    "    return data_arr\n",
    "\n",
    "\n",
    "predictors = transform_nodes_dataframe(categorical_mapping, node_data)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aadt_model(num_sites, num_days, num_predictors, predictors, data):\n",
    "    betas = pyro.sample('betas', dist.Normal(torch.zeros(num_predictors), 10 * torch.ones(num_predictors)))\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(-5, 3)).expand(num_sites, num_days)\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            thetas = predictors @ betas\n",
    "            thetas = thetas + epsilon\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(thetas)), obs=data) \n",
    "\n",
    "    return accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = pyro.sample('betas', dist.Normal(torch.zeros(2), 10 * torch.ones(2))).unsqueeze(-1).expand(10, 4,2,1).squeeze()\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadt_model_guide = autoguide.AutoDiagonalNormal(aadt_model)\n",
    "optimizer = optim.Adam({'lr': .05})\n",
    "num_iters = 5000\n",
    "\n",
    "svi = SVI(aadt_model, aadt_model_guide, optimizer,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "predictors_tens = torch.tensor(predictors).float()\n",
    "for i in range(num_iters): \n",
    "    print(train_mat_tens.size())\n",
    "    elbo = svi.step(num_sites, num_days, 2, predictors_tens, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e+00, 6.53200000e+03, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 6.53200000e+03, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 6.53200000e+03, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 6.53200000e+03, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 6.53200000e+03, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 6.53200000e+03, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 9.97433333e+03, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 9.97433333e+03, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 9.97433333e+03, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.97433333e+03, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 9.97433333e+03, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 9.97433333e+03, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 2.07126667e+04, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 2.07126667e+04, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 2.07126667e+04, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 2.07126667e+04, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 2.07126667e+04, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 2.07126667e+04, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e+00, 9.60300000e+03, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 9.60300000e+03, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 9.60300000e+03, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.60300000e+03, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 9.60300000e+03, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 9.60300000e+03, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 6.26400000e+03, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 6.26400000e+03, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 6.26400000e+03, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 6.26400000e+03, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 6.26400000e+03, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 6.26400000e+03, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]],\n",
       "\n",
       "       [[1.00000000e+00, 8.20357500e+04, 8.95000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 2.90000000e+01],\n",
       "        [1.00000000e+00, 8.20357500e+04, 1.81200000e+01, 1.30000000e-01,\n",
       "         0.00000000e+00, 3.00000000e+01],\n",
       "        [1.00000000e+00, 8.20357500e+04, 1.96900000e+01, 2.20000000e-01,\n",
       "         7.10000000e+00, 1.60000000e+01],\n",
       "        ...,\n",
       "        [1.00000000e+00, 8.20357500e+04, 1.29700000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 4.30000000e+01],\n",
       "        [1.00000000e+00, 8.20357500e+04, 1.20800000e+01, 3.60000000e-01,\n",
       "         0.00000000e+00, 3.90000000e+01],\n",
       "        [1.00000000e+00, 8.20357500e+04, 1.70000000e+01, 0.00000000e+00,\n",
       "         0.00000000e+00, 3.90000000e+01]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_weather_predictors(predictors, df):\n",
    "    df = df.copy()\n",
    "    new_predictors = np.zeros((predictors.shape[0], predictors.shape[1], 6))\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    for elem in df.itertuples():\n",
    "        idx = (elem.datetime - FIRST_DAY).days\n",
    "        \n",
    "        if idx >= 0 and idx < new_predictors.shape[1]:\n",
    "            new_predictors[:, idx, 2:] = [elem.wind, elem.precipitation, elem.snow_depth, elem.average_temperature]\n",
    "    new_predictors[:, :, :2] = predictors\n",
    "    return new_predictors\n",
    "weather_data = pd.read_csv('../data/weather.csv')\n",
    "add_weather_predictors(predictors, weather_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
