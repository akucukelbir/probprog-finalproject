{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import helper_functions.ppc as ppc\n",
    "import pyro\n",
    "from pyro import plate\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.autoguide as autoguide\n",
    "from pyro.infer import MCMC, NUTS, SVI, Trace_ELBO, Predictive\n",
    "import pyro.optim as optim\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import folium\n",
    "import seaborn as sns\n",
    "from folium.plugins import FastMarkerCluster, HeatMap\n",
    "\n",
    "FIRST_DAY = datetime.datetime(2014,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will fit a simple linear model to our data to see how it performs. This will consist of no structured componenets, nor any additional covariates or predictors. It will be used as the baseline with respect to which we can evaluate all of our additional models. To do so first we will take a quick look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``datetime`` and ``node`` cols \n",
    "    Into [day,node] nd.array where arr[i][j] indicates\n",
    "    accidents in location i at j day \n",
    "    Returns accident array\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    categorical = pd.Categorical(df['node'])\n",
    "    codes = categorical.codes\n",
    "\n",
    "    num_days = (df['datetime'].iloc[-1] - FIRST_DAY).days + 1\n",
    "    num_nodes = len(categorical.categories)\n",
    "\n",
    "    data_arr = np.zeros((num_nodes, num_days))\n",
    "    for elem, i in zip(df.itertuples(),range(len(df))): \n",
    "        data_arr[codes[i]][(elem.datetime - FIRST_DAY).days] += 1\n",
    "    \n",
    "    category_mapping = {}\n",
    "    for node, idx in zip(categorical, categorical.codes):\n",
    "        category_mapping[node] = idx \n",
    "    \n",
    "    return num_days, category_mapping, data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_filename = '../data/manhattan_accidents_node_data.csv'\n",
    "node_filename = '../data/nodes_roads.csv'\n",
    "\n",
    "\n",
    "data = pd.read_csv(accident_filename)\n",
    "node_data = pd.read_csv(node_filename)\n",
    "\n",
    "#Only consider accident with node that have corresponding AADT\n",
    "data = data[data['node'].isin(list(node_data['nodes'].unique()))]\n",
    "\n",
    "#Only consider accidents after 2014\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data = data[data['datetime'] >= FIRST_DAY]\n",
    "\n",
    "#This is done to get rid of pesky column. \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can divide the data between a training and test set. To do this we can simply take the time up to year 2016 and then consider everything else validation.  Moreover we will re-organize our data so that we can work with it a comfortable way later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_days, categorical_mapping, data_mat = transform_dataframe(data)\n",
    "\n",
    "num_years_test = 2\n",
    "num_years_train = 3\n",
    "\n",
    "index_train = 365 * num_years_train\n",
    "index_valid = index_train + 365 * num_years_test\n",
    "\n",
    "train_mat = data_mat[:, :index_train]\n",
    "test_mat = data_mat[:, index_train: index_valid]\n",
    "data_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data that we have we can plot several visualizations. First we will plot the points in the map so that we can see what type of data we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_child(FastMarkerCluster(subset))\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being able to visualize the points independently it is also helpful to visualize the points in a heat map to understand if there are any spatial correlations that we should be mindful of. As the map shows, there are definitely some areas that seem to be more likely to have accidents than others. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[40.7, -74.05], zoom_start=10)\n",
    "\n",
    "subset = data[['latitude','longitude']][:].values.tolist()\n",
    "m.add_children(HeatMap(subset, radius = 7.5))\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to understand and which will be key later on is whether there exists some general trends on accidents through time. Of course, it is possible that for some sites some of these trends are present while for some others these trends don't exist. However, for the moment we will only consider whether for manhattan as whole these trends exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_accidents = np.sum(data_mat, axis = 0)\n",
    "smooth_accidents = signal.savgol_filter(time_accidents,61, 3)\n",
    "time = list(range(len(time_accidents)))\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time,y=time_accidents, mode='lines',name='Raw accidents'))\n",
    "fig.add_trace(go.Scatter(x=time,y=smooth_accidents,mode='lines',name='Smooth accidents'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'mean accidents':mean_accidents}), x='mean accidents', kind = 'kde')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accidents = np.sum(train_mat, axis = 1)/len(train_mat)\n",
    "sns.displot(pd.DataFrame({'log mean accidents':np.log(mean_accidents + 0.000000000001/len(mean_accidents))}), x='log mean accidents', kind = 'kde', ax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify our model. Our first model is that both the risk and exposure at each individual site will be constant through time. This is a very crude approximation but it will serve as a simple baseline that we can use for our evaluation. Mathematically, we can state is as follows. Let $j \\in [n]$ where $n$ denotes the number of days. Let $i \\in [m]$ where $m$ denotes the number of sites. Let $Y_{ij}$ the number of accidents at site $i$ during during day $j$. Then we assume that \n",
    "\\begin{align*}\n",
    "\\beta &\\sim \\mathcal{N}(0,I_m)\\\\\n",
    "Y_{ij} &\\sim \\text{Poisson}(\\exp(\\beta_i))\n",
    "\\end{align*}\n",
    "Having specified this, we can simply write down our model and then do inference. Because the betas are independent here, then it would make sense to specify a guide where all of the parameters are normal. This can be done quite easily with an autoguide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sites = train_mat.shape[0]\n",
    "num_days = train_mat.shape[1]\n",
    "\n",
    "def base_model(num_sites, num_days, data):\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
    "        epsilon = pyro.sample('epsilon', dist.Normal(-5, 3))\n",
    "        with plate('days', size=num_days, dim=-1):\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(epsilon)), obs=data)\n",
    "            \n",
    "    return accidents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = autoguide.AutoDiagonalNormal(base_model)\n",
    "optimizer = optim.Adam({'lr': .05})\n",
    "num_iters = 500\n",
    "\n",
    "svi = SVI(base_model, guide, optimizer,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'Loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='Loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the next step is to import predictor data. This would include both intersection-level data such as AADT and day-level data such as weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_nodes_dataframe(categorical_mapping, df): \n",
    "    \"\"\"\n",
    "    Transforms dataframe with ``node`` and corresponding predictor cols \n",
    "    Into [day,node,predictor] nd.array where arr[j][i][k] indicates\n",
    "    predictor k in location i at j day \n",
    "    Returns predictors tensor\n",
    "    \"\"\"\n",
    "    #Categories for classifying the codes\n",
    "    df = df.copy()\n",
    "    \n",
    "    df = df.replace({'nodes': categorical_mapping})\n",
    "\n",
    "    num_nodes = len(categorical_mapping.keys())\n",
    "    \n",
    "    data_arr = np.zeros((num_nodes, num_days, 2))\n",
    "    for i in range(len(df)): \n",
    "        if df['nodes'][i] >= 0 and df['nodes'][i] <= data_arr.shape[0]:\n",
    "            data_arr[df['nodes'][i], :, 0] = 1\n",
    "            data_arr[df['nodes'][i], :, 1] = df['Count_mean'][i]\n",
    "    return data_arr\n",
    "\n",
    "\n",
    "predictors = transform_nodes_dataframe(categorical_mapping, node_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we use now is gonna be different. Before, we had all of our risk and exposure at a crash site modeled by one term per site. Here we introduce several modifications and choose the following model instead. As before, let $Y_{ij}$ denote the number of car crashes at site $i$ on day $j$. Now we introduce a variable $theta_{ij}$  which we will call the risk at site $i$ on day $j$ of a crash and a variable $\\lambda_{ij}$ which we call the exposure at site $i$ on day $j$. This exporsure, is associated with the number of cars that pass through a given site on a day. Intuitively, the more cars that pass through some site on a day. Our model then is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors[:,:,1] = np.log(predictors[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aadt_model(num_sites, num_days, num_predictors,predictors, data=None):\n",
    "    betas = pyro.sample('betas', dist.Normal(torch.zeros(num_predictors), 10 * torch.ones(num_predictors)))\n",
    "    thetas = predictors @ betas\n",
    "    with plate('sites', size=num_sites, dim=-2):\n",
<<<<<<< HEAD
    "        epsilon = pyro.sample('epsilon', dist.Normal(0, 10)).expand(num_sites, num_days)\n",
=======
    "        epsilon = pyro.sample('epsilon', dist.Normal(-5, 3)).expand(num_sites, num_days)\n",
>>>>>>> origin
    "        with plate('days', size=num_days, dim=-1):\n",
    "            #thetas = thetas + epsilon\n",
    "            accidents = pyro.sample('accidents', dist.Poisson(torch.exp(thetas)), obs=data) \n",
    "\n",
    "    return accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aadt_model_guide = autoguide.AutoDiagonalNormal(aadt_model)\n",
    "optimizer = optim.Adam({'lr': .05})\n",
    "num_iters = 500\n",
    "\n",
    "svi = SVI(aadt_model, aadt_model_guide, optimizer,loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "losses = []\n",
    "train_mat_tens = torch.tensor(train_mat)\n",
    "predictors_tens = torch.tensor(predictors).float()\n",
    "covariates = predictors_tens\n",
    "for i in range(num_iters): \n",
    "    elbo = svi.step(num_sites, num_days, 2, predictors_tens, train_mat_tens)\n",
    "    losses.append(elbo)\n",
    "    if i % 50 == 0: \n",
    "        print(\"In step {} the Elbo is {}\".format(i,elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_df = pd.DataFrame({'Iteration': list(range(len(losses))), 'log-loss': np.log(losses[:])})\n",
    "fig = px.line(elbo_df, x='Iteration', y='log-loss', title='Elbo')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should go in an independent file \n",
    "from collections import defaultdict\n",
    "from pyro import poutine\n",
    "from pyro.poutine.util import prune_subsample_sites\n",
    "import warnings\n",
    "\n",
    "class Predict(torch.nn.Module):\n",
    "    def __init__(self, model, guide, num_samples=800):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        total_samples = {}\n",
    "        for i in range(self.num_samples): \n",
    "            guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)\n",
    "            model_trace = poutine.trace(poutine.replay(self.model, guide_trace)).get_trace(*args, **kwargs)\n",
    "            for site in prune_subsample_sites(model_trace).stochastic_nodes:\n",
    "                if site not in total_samples: \n",
    "                    total_samples[site] = []\n",
    "                total_samples[site].append(model_trace.nodes[site]['value'])\n",
    "        for key in total_samples.keys():\n",
    "            total_samples[key] = torch.stack(total_samples[key])\n",
    "\n",
    "        return total_samples\n",
    "    \n",
    "\n",
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\": torch.std(v, 0),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = Predict(aadt_model,aadt_model_guide, 800)\n",
    "samples = predict(num_sites, num_days, num_predictors, predictors_tens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(samples['betas'][:,0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['epsilon'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = torch.distributions.Categorical(torch.ones(4,num_sites)/num_sites).sample()\n",
    "print(subset)\n",
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(),train_mat,(2,2),subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logAADT =  predictors[:,0,1]\n",
    "total_crash = np.sum(train_mat,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(logAADT, np.log(total_crash),alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_predictors(predictors, df):\n",
    "    df = df.copy()\n",
    "    new_predictors = np.zeros((predictors.shape[0], predictors.shape[1], 6))\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    for elem in df.itertuples():\n",
    "        idx = (elem.datetime - FIRST_DAY).days\n",
    "        \n",
    "        if idx >= 0 and idx < new_predictors.shape[1]:\n",
    "            new_predictors[:, idx, 2:] = [elem.wind, elem.precipitation, elem.snow_depth, elem.average_temperature]\n",
    "    new_predictors[:, :, :2] = predictors\n",
    "    return new_predictors\n",
    "weather_data = pd.read_csv('../data/weather.csv')\n",
    "new_predictors = add_weather_predictors(predictors, weather_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
