{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from helper_functions import ppc\n",
    "from models import models, guides\n",
    "from plots import plots \n",
    "import predictors\n",
    "from models import posterior\n",
    "import pyro.contrib.autoguide as autoguideΩ\n",
    "\n",
    "import torch \n",
    "from torch.distributions.constraints import positive\n",
    "import numpy as np\n",
    "import pandas \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import data\n",
    "from pyro import plate, poutine\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Trace_ELBO, config_enumerate, infer_discrete\n",
    "import pyro.optim as optim \n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Crash Prediction in Manhattan using Variational Inference\n",
    "\n",
    "Over 1.73 million crash incidents have been reported in NYC since 2012. The magnitude of this number indicates the importance of developing and understanding of the patterns that drive this phenomenon. With the aim of achieving this objective we developed various probability models that describe how the phenomenon occurs on a day to day level in different regions within Manhattan. We perform inference on our model using Stochastic Variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "We used four types of data. \n",
    "1. Location and time of all car crashes reported in Manhattan from [NYC OpenData](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)\n",
    "\n",
    "2. Daily average temperature, wind speed, rain volume and snow depth at JFK airport collected from [NCDC](https://www.ncdc.noaa.gov/cdo-web/search)\n",
    "\n",
    "3. Intersection location and characteristics for all intersection of manhattan from [Kaggle](https://www.kaggle.com/crailtap/street-network-of-new-york-in-graphml)\n",
    "\n",
    "4. Annual average daily traffic for all road segments in manhattan for which this information was available, taken from [NY government](https://data.ny.gov/Transportation/Annual-Average-Daily-Traffic-AADT-Beginning-1977/6amx-2pbv) \n",
    "\n",
    "\n",
    "Below we plot some graphs that show different aspects of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents, preds  = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_mean_log_mean(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below plots accidents through time and uses a smoother to detect trends through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_time_series(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "To aggregate the data we decided to map each accident to the nearest intersection. We decided to use the average AADT over the period from 2014 to 2019 for each road as there was missing data. We matched the AADT of the roads to the intersections by proximity. Roads without AADT data available were remove. For interpretability and inference purposes we used normalized $\\log$(AADT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_names = ['wind','snow_depth', 'temperature','precipitation']\n",
    "kappa = 0.55\n",
    "t_0 = 3\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=3000)\n",
    "plots.plot_svi_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 300)\n",
    "samples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 300)\n",
    "samples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred_names)\n",
    "plots.plot_betas(samples['betas'].detach().numpy(), pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(samples['accidents'].detach().numpy(),accidents, window=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(), accidents, shape=(2,2),subset=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "The main model that we used is a poisson log-linear model. Let $m$ denote the number of days, $n$ the number of sites or regions, and $k$ the number of predictors. Then we let  $Y_{ij} $ denote the number of car crashes in region $i$ on day $j$.  Call $\\beta \\in \\mathbb{R}^k$ the regression coefficients and $X_{ij} \\in \\mathbb{R}^k$ the predictors for site $i$ on day $k$. We then assume that the data is generated from the following model \n",
    "\n",
    "\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, 10^2 * I_n)\\\\\n",
    "\\beta \\sim \\mathcal{N}(0, 5^2 * I_k)\\\\\n",
    "\\log(\\theta_{ij}) = X_{ij}^\\top \\beta + \\epsilon_{i}\\\\\n",
    "Y_{ij} \\sim \\text{Poisson}(\\theta_{ij})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Investigations: Inference Method and Model Choices\n",
    "\n",
    "Before working on the real data, we implemented our main models and fit them to synthetically generated data from the model itself. We found that the models were able to recover the correct beta’s even when the generated data had similar sparsity as the real data. We learned two things from this:\n",
    "\n",
    "- Initially, we were planning on fitting a conditional autoregressive model using spatial correlation. However, we found this to be prohibitively computationally expensive regardless of inference method due to computations involving large spatial matrices. Due to the large number of intersections, this was not a feasible model. As such, we decided to use the poisson-lognormal model.\n",
    "\n",
    "- Initially, we wanted to use MCMC as our inference method but we soon found this to be computationally infeasible due to the large amount of data. Instead, we choice to use stochastic variational inference using Pyro's automatic guide generation. \n",
    "\n",
    "## Base model \n",
    "For our base model we assume that $k = 1$. This is equivalent to assuming no structure across nodes and simply modelling every region independently and an intercept. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = []\n",
    "kappa = 0.55\n",
    "t_0 = 20\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=1000)\n",
    "plots.plot_svi_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 300)\n",
    "bmsamples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPC \n",
    "\n",
    "We now check whether our model is able to generate similar data to the actual data. To do so, we conduct two posterior predictive checks:\n",
    "\n",
    "- The actual number of accidents at an intersection and the empirical distribution constructed from the posterior samples. We expect a good model to have the mean of the empirical distribution to coincide with the actual data.\n",
    "- The time series of aggregate accidents and the corresponding quartiles implied by the model. We expect a good model to take into account any time related trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(), accidents, (2,2), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_max(samples['accidents'].detach().numpy(), accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the disparity with maximum\n",
    "As the plots above show, the maximum number of crashes at a particular site is not captured well. Thus, we decided to fit negative bionomial model as it allows for an excess kurtosis greater than that of a poisson. \n",
    "We modeled the mean with the same structure as our main model and used a uniform prior over the parameter $p$ (success probability in each trial). We fit a model below with just a constant for illustration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_days = 365\n",
    "selection = predictors.get_some_predictors(preds, [])[:,:num_days]\n",
    "kappa = 0.5 + 1e-10\n",
    "t_0 = 40\n",
    "model_args = {'num_sites':accidents.shape[0],\n",
    "             'num_days':num_days,\n",
    "              'num_predictors':selection.shape[-1], \n",
    "              'predictors': torch.Tensor(selection),\n",
    "             'data': torch.Tensor(accidents[:,:num_days])}\n",
    "\n",
    "losses = models.train(models.negative_binomial_log_linear_model,\n",
    "                      guides.negative_binomial_guide,\n",
    "                      kappa = kappa,\n",
    "                      t_0 = t_0,\n",
    "                      model_args = model_args,\n",
    "                      max_iters = 10000\n",
    "                     )\n",
    "plots.plot_svi_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = posterior.Predict(models.negative_binomial_log_linear_model, guides.negative_binomial_guide, 400)\n",
    "binsamples = predict(accidents.shape[0], num_days, selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_total_distributions(binsamples['accidents'].detach().numpy(),accidents[:,:num_days],(2,2),12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_max(binsamples['accidents'].detach().numpy() ,accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify whether these results made sense we decided to compare the variance to the mean across sites. We determined that they were a poor approximation to reality. Moreover, we found three outliers which accounter for days with more than six crashes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_mean_variance_line(binsamples['accidents'].detach().numpy()[np.random.randint(0,400)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_mean_variance_line(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['aadt','is_intersection']\n",
    "kappa = 0.5\n",
    "t_0 = 20\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=3000)\n",
    "plots.plot_svi_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 400)\n",
    "insamples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_betas(insamples['betas'].detach().numpy(), pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(insamples['accidents'].detach().numpy(),accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather model\n",
    "\n",
    "This prompts us to add predictors related to the day. We add temperature, snow depth, wind and precipitation as predictors as these are natural factors that one would expect to affect accident rates. After fitting the model using SVI, we apply the above PPC again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the second criteria is better captured by this model but not perfectly. Due to the non-stationary confidence interval implied by our modified model, it is better able to capture the time-trends in the aggregate data. \n",
    "\n",
    "We introduce a further check to test whether a given categorical daily predictor affects accident rates. This check displays a histogram of the difference between the mean accident for each node across days when the categorical predictor is 1 and the corresponding mean when it is 0. If said predictor does indeed affect the accident rate, we expect the histogram to have non-zero mean and be skewed . We can also conduct this check with non-categorical predictors by introducing a cutoff for the purpose of this check. To test whether the model is able to capture this, we plot the empirical distribution of the skew of the generated data and also plot a single sample to see whether the shapes of the histograms are similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['aadt','is_intersection', 'wind','snow_depth', 'temperature','precipitation']\n",
    "kappa = 0.55\n",
    "t_0 = 30\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=2000)\n",
    "plots.plot_svi_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 400)\n",
    "wsamples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(wsamples['accidents'].detach().numpy(), accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the samples of the cofficients from the posterior. We see that rain has a significant positive coefficient indicating that rain increases rate\n",
    "\n",
    "However, doing PPC with node data, we notice that our model is not able to capture differences in mean of nodes with high and low AADT. Thus, we need to add more predictors to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complication with SVI\n",
    "\n",
    "When fitting the above model we noticed that running inference multiple times led to inconsistent results from the PPC. Sometimes, the trend lines would fit well and other times it would not. After studying the corresponding ELBO curves, we concluded that this was due to the ELBO not converging even after a significant number of iterations (~5000 iterations). We found that this was due to bad initialization of the guide. To prevent this, we initialize the guide a hundred times and select the initialization based on the lowest ELBO. This led us to consistent results from the PPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Poisson-Lognormal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we want to add AADT for the node and whether the node is an intersection of 3 or more roads (as sometimes the node is simply a corner). After fitting the model, we see that the PPC sample has a similar empirical distribution of the mean difference as the actual data. Thus, this model is better able to capture node differeneces. However, we find it surprising that the AADT has a negative coefficient because intuitively we expected higher traffic to translate to greater accidents. Perhaps, this is due to higher traffic intersections having certain characteristics that make accidents less likely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_difference(data, selector, axis):\n",
    "    selected_idx = np.argwhere(selector)\n",
    "    unselected_idx = np.argwhere(np.ones(data.shape[axis]) - selector)\n",
    "    if axis == 1:\n",
    "        sel = data[:, selected_idx]\n",
    "        unsel = data[:, unselected_idx]\n",
    "    else:\n",
    "        sel = data[selected_idx, :]\n",
    "        unsel = data[unselected_idx, :]\n",
    "    selected_means = np.sum(sel, axis = axis)/len(selected_idx)\n",
    "    unselected_means = np.sum(unsel, axis = axis)/len(unselected_idx)\n",
    "    return np.squeeze(selected_means - unselected_means)\n",
    "\n",
    "def compute_mean_sample_mean_difference(data, selector, axis):\n",
    "    return compute_mean_difference(np.sum(data, axis=0)/data.shape[0], selector, axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skew(compute_mean_difference(samples['accidents'][0].detach().numpy(),preds[0,:,4]>0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waccidentsv = torch.var(wsamples['accidents'], axis=1)\n",
    "accidentssv = torch.var(samples['accidents'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(accidentssv.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
