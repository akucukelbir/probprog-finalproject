{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from helper_functions import ppc\n",
    "from models import models\n",
    "from plots import plots \n",
    "import predictors\n",
    "from models import posterior\n",
    "\n",
    "import torch \n",
    "from torch.distributions.constraints import positive\n",
    "import numpy as np\n",
    "import pandas \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import data\n",
    "from pyro import plate, poutine\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Trace_ELBO, config_enumerate, infer_discrete\n",
    "import pyro.optim as optim \n",
    "\n",
    "accident_filename = '../data/accident/processed/manhattan.csv'\n",
    "node_filename = '../data/intersection/processed/data.csv'\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Crash Prediction in Manhattan using Variational Inference\n",
    "\n",
    "Over 1.73 million crash incidents have been reported in NYC since 2012. The magnitude of this number indicates the importance of developing and understanding of the patterns that drive this phenomenon. With the aim of achieving this objective we developed various probability models that describe how the phenomenon occurs on a day to day level in different regions within Manhattan. We perform inference on our model using Stochastic Variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "We used four types of data. \n",
    "1. Location and time of all car crashes reported in Manhattan from [NYC OpenData](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)\n",
    "\n",
    "2. Daily average temperature, wind speed, rain volume and snow depth at JFK airport collected from [NCDC](https://www.ncdc.noaa.gov/cdo-web/search)\n",
    "\n",
    "3. Intersection location and characteristics for all intersection of manhattan from [Kaggle](https://www.kaggle.com/crailtap/street-network-of-new-york-in-graphml)\n",
    "\n",
    "4. Annual average daily traffic for all road segments in manhattan for which this information was available, taken from [NY government](https://data.ny.gov/Transportation/Annual-Average-Daily-Traffic-AADT-Beginning-1977/6amx-2pbv) \n",
    "\n",
    "A plot of the car crash data can be found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " accidents, preds  = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_time_series(accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_mean_log_mean(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "To aggregate the data we decided to map each accident to the nearest intersection. We decided to use the average AADT over the period from 2014 to 2019 for each road as the data was not available for each individual year in most cases. Each defined region was assigned the maximum average AADT of all roads within 10 m of the intersection location. Some roads had no AADT data available for all these years and thus they were removed. For interpretability and inference purposes we used normalized $\\log$(AADT) instead of AADT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['wind','snow_depth', 'temperature','precipitation']\n",
    "kappa = 0.50000001\n",
    "t_0 = 3\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=3000)\n",
    "plots.plot_svi_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 300)\n",
    "samples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred_names)\n",
    "plots.plot_betas(samples['betas'].detach().numpy(), pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(samples['accidents'].detach().numpy(),accidents, window=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(), accidents, shape=(2,2),subset=[1328,10,48,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "The main model that we used is a poisson log-linear model. Let $m$ denote the number of days, $n$ the number of sites or regions, and $k$ the number of predictors. Then we let  $Y_{ij} $ denote the number of car crashes in region $i$ on day $j$.  Call $\\beta \\in \\mathbb{R}^k$ the regression coefficients and $X_{ij} \\in \\mathbb{R}^k$ the predictors for site $i$ on day $k$. We then assume that the data is generated from the following model \n",
    "\n",
    "\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, 10 * I_n)\\\\\n",
    "\\beta \\sim \\mathcal{N}(0, 5 * I_k)\\\\\n",
    "\\log(\\theta_{ij}) = X_{ij}.T \\beta + \\epsilon_{i}\\\\\n",
    "Y_{ij} \\sim Poisson(\\theta_{ij})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Investigations: Inference Method and Model Choices\n",
    "\n",
    "Before working on the real data, we implemented our main models and fit them to synthetically generated data from the model itself. We found that the models were able to recover the correct betaâ€™s even when the generated data had similar sparsity as the real data. We learned two things from this:\n",
    "\n",
    "- Initially, we were planning on fitting a conditional autoregressive model using spatial correlation. However, we found this to be prohibitively computationally expensive regardless of inference method due to computations involving large spatial matrices. Since we were looking at large number of intersections, this was not a feasible model. As such, we decided to use the poisson lognormal model.\n",
    "\n",
    "- Initially, we wanted to use MCMC as our inference method but we soon found this to be computationally infeasible due to the large amount of data. Instead, we choice to use stochastic variational inference using Pyro's automatic guide generation. \n",
    "\n",
    "## Base model \n",
    "For our base model we assume that $k = 0$. This is equivalent to assuming no structure across nodes and simply modelling every region independently. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPC \n",
    "\n",
    "We now check whether our model is able to generate similar data to the actual data. To do so, we conduct two posterior predictive checks:\n",
    "\n",
    "- The actual number of accidents at an intersection and the empirical distribution constructed from the posterior samples. We expect a good model to have the mean of the empirical distribution to coincide with the actual data.\n",
    "- The time series of aggregate accidents and the corresponding quartiles implied by the model. We expect a good model to take into account any time related trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather model\n",
    "\n",
    "This prompts us to add predictors related to the day. We add temperature, snow depth, wind and precipitation as predictors as these are natural factors that one would expect to affect accident rates. After fitting the model using SVI, we apply the above PPC again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the second criteria is better captured by this model but not perfectly. Due to the non-stationary confidence interval implied by our modified model, it is better able to capture the time-trends in the aggregate data. \n",
    "\n",
    "We introduce a further check to test whether a given categorical daily predictor affects accident rates. This check displays a histogram of the difference between the mean accident for each node across days when the categorical predictor is 1 and the corresponding mean when it is 0. If said predictor does indeed affect the accident rate, we expect the histogram to have non-zero mean and be skewed . We can also conduct this check with non-categorical predictors by introducing a cutoff for the purpose of this check. To test whether the model is able to capture this, we plot the empirical distribution of the skew of the generated data and also plot a single sample to see whether the shapes of the histograms are similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complication with SVI\n",
    "\n",
    "When fitting the above model we noticed that running inference multiple times led to inconsistent results from the PPC. Sometimes, the trend lines would fit well and other times it would not. After studying the corresponding ELBO curves, we concluded that this was due to the ELBO not converging even after a significant number of iterations (~5000 iterations). We found that this was due to bad initialization of the guide. To prevent this, we initialize the guide a hundred times and select the initialization based on the lowest ELBO. This led us to consistent results from the PPC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_difference(data, selector, axis):\n",
    "    selected_idx = np.argwhere(selector)\n",
    "    unselected_idx = np.argwhere(np.ones(data.shape[axis]) - selector)\n",
    "    if axis == 1:\n",
    "        sel = data[:, selected_idx]\n",
    "        unsel = data[:, unselected_idx]\n",
    "    else:\n",
    "        sel = data[selected_idx, :]\n",
    "        unsel = data[unselected_idx, :]\n",
    "    selected_means = np.sum(sel, axis = axis)/len(selected_idx)\n",
    "    unselected_means = np.sum(unsel, axis = axis)/len(unselected_idx)\n",
    "    return np.squeeze(selected_means - unselected_means)\n",
    "\n",
    "def compute_mean_sample_mean_difference(data, selector, axis):\n",
    "    return compute_mean_difference(np.sum(data, axis=0)/data.shape[0], selector, axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skew(compute_mean_difference(samples['accidents'][0].detach().numpy(),preds[0,:,4]>0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
