{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from helper_functions import ppc\n",
    "from models import models\n",
    "from plots import plots \n",
    "import predictors\n",
    "from models import posterior\n",
    "\n",
    "import torch \n",
    "from torch.distributions.constraints import positive\n",
    "import numpy as np\n",
    "import pandas \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import data\n",
    "from pyro import plate, poutine\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, Trace_ELBO, config_enumerate, infer_discrete\n",
    "import pyro.optim as optim \n",
    "\n",
    "accident_filename = '../data/accident/processed/manhattan.csv'\n",
    "node_filename = '../data/intersection/processed/data.csv'\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Crash Prediction in Manhattan using Variational Inference\n",
    "\n",
    "Over 1.73 million crash incidents have been reported in NYC since 2012. The magnitude of this number indicates the importance of developing and understanding of the patterns that drive this phenomenon. With the aim of achieving this objective we developed various probability models that describe how the phenomenon occurs on a day to day level in different regions within Manhattan. We perform inference on our model using Black Box Variational inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "We used four types of data. \n",
    "1. Location and time of all car crashes reported in Manhattan. \n",
    "2. Daily average temperature, wind speed, rain volume and snow depth collected from the JFK airport.\n",
    "3. Intersection location and characteristics (ASIF ADD HERE) for all intersection of manhattan\n",
    "4. Anual average daily traffic for all road segments in manhattan for which this information was available. \n",
    "\n",
    "A plot of the car crash data can be found below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " accidents, preds  = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_time_series(accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.make_mean_log_mean(accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "To aggregate the data we decided to map each accident to the nearest intersection. This divided the space into a set of regions similar to those obtained by a voronoi diagram with centers corresponding to the intersections. We decided to use the average AADT over the period from 2014 to 2019 for each road as the data was not available for each individual year in most cases. Each defined regions was assigned the maximum average AADT of all roads within 10 m of the intersection location. Some roads had no AADT data available for each of these years and thus some regions were assigned no AADT. These regions were discarded so that our probability models were comparable to each other. For interpretability and inference purposes we used $\\log(AADT)$ instead of AADT and all values were divided by the absolute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_names = ['wind','snow_depth', 'temperature','precipitation']\n",
    "kappa = 0.50000001\n",
    "t_0 = 3\n",
    "loss, guide = models.train_log_linear_random_init(accidents,\n",
    "                        preds,\n",
    "                        pred_names, \n",
    "                        kappa=kappa,\n",
    "                        t_0=t_0, \n",
    "                        max_iters=3000)\n",
    "plots.plot_svi_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = predictors.get_some_predictors(preds, pred_names)\n",
    "predict = posterior.Predict(models.log_linear_model, guide, 300)\n",
    "samples = predict(accidents.shape[0], accidents.shape[1], selection.shape[2], torch.Tensor(selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(pred_names)\n",
    "plots.plot_betas(samples['betas'].detach().numpy(), pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_time_trend(samples['accidents'].detach().numpy(),accidents, window=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc.plot_total_distributions(samples['accidents'].detach().numpy(), accidents, shape=(2,2),subset=[1328,10,48,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "The main model that we used is a poisson log-linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_difference(data, selector, axis):\n",
    "    selected_idx = np.argwhere(selector)\n",
    "    unselected_idx = np.argwhere(np.ones(data.shape[axis]) - selector)\n",
    "    if axis == 1:\n",
    "        sel = data[:, selected_idx]\n",
    "        unsel = data[:, unselected_idx]\n",
    "    else:\n",
    "        sel = data[selected_idx, :]\n",
    "        unsel = data[unselected_idx, :]\n",
    "    selected_means = np.sum(sel, axis = axis)/len(selected_idx)\n",
    "    unselected_means = np.sum(unsel, axis = axis)/len(unselected_idx)\n",
    "    return np.squeeze(selected_means - unselected_means)\n",
    "\n",
    "def compute_mean_sample_mean_difference(data, selector, axis):\n",
    "    return compute_mean_difference(np.sum(data, axis=0)/data.shape[0], selector, axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skew(compute_mean_difference(samples['accidents'][0].detach().numpy(),preds[0,:,4]>0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
